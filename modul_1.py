# -*- coding: utf-8 -*-
"""01_Preprocessing_step [DONE].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZjdrfRUOOcUXke_EiQ6AR80dX4BzEPMu

# Load Data
"""

import pandas as pd
fulldata = pd.read_csv('dataset.csv')
fulldata.head()

dataraw = fulldata[['komentar']]
dataraw.head()

dataraw.info()

"""# Preprocessing"""

import re
import nltk
from nltk.corpus import stopwords

#Menghapus karakter yang tidak diperlukan
def remove_nonAlphabet(text):
    pattern = r'[^a-zA-Z\s]'
    text = re.sub(pattern, '', text)
    return text

#Mengubah teks menjadi lowercase
def convert_toLowercase(text):
    return text.lower()

#Tokenisasi
def tokenize_text(text):
    tokens = nltk.word_tokenize(text)
    return tokens

#Menghapus stop words
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('indonesian'))
    filtered_tokens = [token for token in tokens if token not in stop_words]
    return filtered_tokens

def removeWords(tokens):
    interjections = set(['ah', 'oh', 'ih', 'up', 'yg', 'yang',
                         'jg', 'g', 'nya', 'ga', 'sih', 'aja',
                         'gitu', 'udah', 'udah', 'kalo', 'jd',
                         'kalo', 'ya', 'tp', 'dr', 'emang',
                         'tdk', 'mah', 'biar', 'dah', 'tau',
                         'pa', 'gimana', 'sdh', 'udh', 'lg',
                         'tu', 'banget', 'dgn', 'mantap',
                         'nih', 'kaya', 'gk', 'bagu', 'tu',
                         'ma', 'd', 'gak', 'klo', 'bgt', 'si',
                         'kali', 'tpi', 'beda', 'coba', 'semoga',
                         'viral', 'wkwk', 'iya', 'alhamdulillah',
                         'allah', 'sm', 'blm', 'tuh', 'dan', 'atau',
                         'serta', 'karena', 'krn', 'saya', 'kamu',
                         'mereka', 'gw', 'bang', 'gua', 'kak', 'bu',
                         'sy', 'orang', 'org', 'ku', 'jokowi', 'haha',
                         'om', 'mbk','bnget', 'kamarudin', 'aq', 'kang',
                         'pk', 'bener', 'adik', 'e', 'setuju','dede',
                         'jokowi', 'ka', 'aa', 'opung','buu', 'luhut',
                         'mba', 'gini', 'k', 'sampe', 'kayak', 'kek',
                         'ni', 'utk', 'y', 'gmn', 'ni', 'bs', 'ny',
                         'mending', 'sesuai', 'bingung', 'suruh',
                         'mas', 'mbak', 'pas', 'wkwkwk', 'bngt',
                         'gtu', 'anj', 'kyk', 'dlu', 'gt', 'anjir',
                         'disuruh', 'min', 'erickthohir', 'hrus',
                         'teu', 'ngk', 'the', 'ngga', 'pdhl', 'nih', 'yah',
                         'ko', 'itu', 'kemana', 'wes', 'knp', 'byk',
                         'dst', '...z', 'smua', 'koq', 'dkk', 'deh',
                         'kan', 'jd', 'yg', 'gua', 'yaa', 'dll', 'ya',
                         'sm', 'gmn', 'g', 'gtw', 'kayaknya', 'udh', 'kayak',
                         'apa', 'mng', 'gk', 'smg', 'ngerti', 'kl',
                         'lh', 'ngak', 'sbg', 'hehe', 'sda', 'jga',
                         'tlg', 'ta', 'da', 'yuk', 'nih', 'yg',
                         'viralkan', 'skrg', 'semangat', 'tru',
                         'ae', 'aamiin', 'm', 'skrng', 'sing',
                         'erick', 'pake', 'terus', 'ad', 'jdi',
                         'trus', 'gue', 'msh', 'lbh', 'krna',
                         'tetep', 'ngomong', 'cepat', 'emg', 'dlm',
                         'sya', 'jgn', 'karna', 'doang']) # Daftar kata-kata seru dalam bahasa Indonesia
    filtered_tokens = [token for token in tokens if token not in interjections]
    return filtered_tokens

# Normalisasi
def replaceWords(tokens):
    word_dict = {
        'bpj': 'bpjs',
        'duit' : 'uang',
        'rb' : 'ribu',
        'jt' : 'juta',
        'pn' : 'pns',
        'rs' : 'rumah sakit',
        'kk' : 'kartu keluarga',
        'mentri' : 'menteri',
    }

    normalized_tokens = [word_dict.get(word, word) for word in tokens]
    return normalized_tokens

def cleansing(text):
    #Menghapus karakter yang tidak diperlukan
    text = remove_nonAlphabet(text)

    #Mengubah teks menjadi lowercase
    text = convert_toLowercase(text)

    #Tokenisasi
    tokens = tokenize_text(text)

    #Menghapus stop words
    tokens = remove_stopwords(tokens)
    tokens = removeWords(tokens)

    #Normalisasi kata
    tokens = replaceWords(tokens)

    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

dataraw['komentar_AfterPrep'] = dataraw['komentar'].apply(cleansing)
dataraw.head()

dataraw.info()

dataraw.to_csv('datasetC1.csv', index=False)

AllDataPrep = pd.read_csv('datasetC1.csv')
AllDataPrep.head()

# hapus baris yang kosong (NaN)
splitdata = AllDataPrep[['komentar_AfterPrep']].dropna()
splitdata.info()

#Hapus Duplicate
splitdata = splitdata.drop_duplicates(subset='komentar_AfterPrep', keep='first')
splitdata.info()

dataclean = splitdata['komentar_AfterPrep']

cleancomment = []
for baris in dataclean:
    if len(baris) > 15:
        cleancomment.append(baris)

cleancomment

dataclean = pd.DataFrame(cleancomment, columns=['komentarClean'])

dataclean.info()

dataclean.head()

"""# Wordcloud"""

#WordCloud Keseluruhan
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Buat objek WordCloud
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='Blues',
    max_words=50,
    stopwords=None
)

teks_anda = ' '.join(dataclean['komentarClean'])
# Generate wordcloud
wordcloud.generate(teks_anda)

# Tampilkan wordcloud
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

dataclean.to_csv('datasetCleaned.csv', index=False)